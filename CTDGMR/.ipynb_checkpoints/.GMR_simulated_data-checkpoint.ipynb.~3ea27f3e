{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gmm_reduction import GMR_clustering, GMR_greedy, GMR_L2, GMR_CWD\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-1.],\n",
      "       [ 1.]]), array([[[1.]],\n",
      "\n",
      "       [[2.]]]), array([0.5, 0.5]))\n",
      "(array([[-1.],\n",
      "       [ 1.]]), array([[[1.]],\n",
      "\n",
      "       [[2.]]]), array([0.5, 0.5]))\n",
      "(array([[-1.],\n",
      "       [ 1.]]), array([[[1.]],\n",
      "\n",
      "       [[2.]]]), array([0.5, 0.5]))\n",
      "(array([[-1.],\n",
      "       [ 1.]]), array([[[1.]],\n",
      "\n",
      "       [[2.]]]), array([0.5, 0.5]))\n",
      "mean [[-1.]\n",
      " [ 1.]]\n",
      "cov [[[1.]]\n",
      "\n",
      " [[2.]]]\n",
      "weight [0.5 0.5]\n",
      "mean [[-0.00291044]\n",
      " [ 0.00185844]]\n",
      "cov [[[2.49853631]]\n",
      "\n",
      " [[2.50092577]]]\n",
      "weight [0.38970095 0.61029905]\n",
      "mean [[-0.00029632]\n",
      " [ 0.00026093]]\n",
      "cov [[[2.49985175]]\n",
      "\n",
      " [[2.5001304 ]]]\n",
      "weight [0.46825299 0.53174701]\n"
     ]
    }
   ],
   "source": [
    "# # greedy algorithm\n",
    "# # 1D case, same support, different weights\n",
    "means = np.array([-1,1,-1,1]).reshape((-1,1))\n",
    "covs = np.array([1,2,1,2]).reshape((-1,1,1))\n",
    "weights = np.array([0.2, 0.3, 0.3, 0.2])\n",
    "print(GMR_greedy(means, covs, weights, 2, \"Salmond\"))\n",
    "print(GMR_greedy(means, covs, weights, 2, \"W\"))\n",
    "print(GMR_greedy(means, covs, weights, 2, \"Runnalls\"))\n",
    "print(GMR_greedy(means, covs, weights, 2, \"Williams\"))\n",
    "\n",
    "\n",
    "# minimum L2 distance GMR\n",
    "# 1D case, same support, different weights\n",
    "means = np.array([-1,1,-1,1]).reshape((-1,1))\n",
    "covs = np.array([1,2,1,2]).reshape((-1,1,1))\n",
    "weights = np.array([0.2, 0.3, 0.3, 0.2])\n",
    "reduced_gmm= GMR_L2(means, covs, weights, 2)\n",
    "reduced_gmm.iterative()\n",
    "print(\"mean\",reduced_gmm.reduced_means)\n",
    "print(\"cov\",reduced_gmm.reduced_covs)\n",
    "print(\"weight\",reduced_gmm.reduced_weights)\n",
    "\n",
    "# print((0.10401147-0.74961755)/(0.10401147+0.74961755),(0.89598853-0.25038245)/(0.89598853+0.25038245))\n",
    "\n",
    "# dphem and hem\n",
    "means = np.array([-1,1,-1,1]).reshape((-1,1))\n",
    "covs = np.array([1,2,1,2]).reshape((-1,1,1))\n",
    "weights = np.array([0.2, 0.3, 0.3, 0.2])\n",
    "reduced_gmm= GMR_clustering(means, covs, weights, 2, method=\"dphem\", init_method=\"Runnalls\")\n",
    "reduced_gmm.iterative()\n",
    "print(\"mean\",reduced_gmm.reduced_means)\n",
    "print(\"cov\",reduced_gmm.reduced_covs)\n",
    "print(\"weight\",reduced_gmm.reduced_weights)\n",
    "\n",
    "\n",
    "\n",
    "means = np.array([-1,1,-1,1]).reshape((-1,1))\n",
    "covs = np.array([1,2,1,2]).reshape((-1,1,1))\n",
    "weights = np.array([0.2, 0.3, 0.3, 0.2])\n",
    "reduced_gmm= GMR_clustering(means, covs, weights, 2, method=\"hem\", init_method=\"Runnalls\")\n",
    "reduced_gmm.iterative()\n",
    "print(\"mean\",reduced_gmm.reduced_means)\n",
    "print(\"cov\",reduced_gmm.reduced_covs)\n",
    "print(\"weight\",reduced_gmm.reduced_weights)\n",
    "\n",
    "\n",
    "\n",
    "# means = np.array([-0.56180209, 2.45164911, -1.77274589, 0.89885554]).reshape((-1,1))\n",
    "# covs = np.array([0.33115079, 0.87928759, 0.52567441, 0.41778262]).reshape((-1,1,1))\n",
    "# weights = np.array([0.33815885, 0.15648689, 0.24376494, 0.26158931])\n",
    "# reduced_gmm= GMR_clustering(means, covs, weights, 2, method=\"dphem\", init_method=\"Runnalls\")\n",
    "# reduced_gmm.iterative()\n",
    "# print(\"mean\",reduced_gmm.reduced_means)\n",
    "# print(\"cov\",reduced_gmm.reduced_covs)\n",
    "# print(\"weight\",reduced_gmm.reduced_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean [[-0.0022127  1.       ]\n",
      " [ 0.0022127  1.       ]]\n",
      "cov [[[1.9999951 0.       ]\n",
      "  [0.        1.       ]]\n",
      "\n",
      " [[1.9999951 0.       ]\n",
      "  [0.        1.       ]]]\n",
      "weight [0.5 0.5]\n",
      "(array([[-1.,  1.],\n",
      "       [ 1.,  1.]]), array([[[1., 0.],\n",
      "        [0., 1.]],\n",
      "\n",
      "       [[1., 0.],\n",
      "        [0., 1.]]]), array([0.5, 0.5]))\n",
      "(array([[-1.,  1.],\n",
      "       [ 1.,  1.]]), array([[[1., 0.],\n",
      "        [0., 1.]],\n",
      "\n",
      "       [[1., 0.],\n",
      "        [0., 1.]]]), array([0.5, 0.5]))\n",
      "(array([[-1.,  1.],\n",
      "       [ 1.,  1.]]), array([[[1., 0.],\n",
      "        [0., 1.]],\n",
      "\n",
      "       [[1., 0.],\n",
      "        [0., 1.]]]), array([0.5, 0.5]))\n",
      "(array([[-1.,  1.],\n",
      "       [ 1.,  1.]]), array([[[1., 0.],\n",
      "        [0., 1.]],\n",
      "\n",
      "       [[1., 0.],\n",
      "        [0., 1.]]]), array([0.5, 0.5]))\n"
     ]
    }
   ],
   "source": [
    "# 2D case, same support, different weights\n",
    "means = np.array([-1,1, 1, 1]*2).reshape((4,2))\n",
    "covs = np.tile(np.eye(2), (4,1,1))\n",
    "weights = np.array([0.2, 0.3, 0.3, 0.2])\n",
    "reduced_gmm= GMR_clustering(means, covs, weights, 2, method=\"dphem\", init_method=\"Runnalls\")\n",
    "reduced_gmm.iterative()\n",
    "print(\"mean\",reduced_gmm.reduced_means)\n",
    "print(\"cov\",reduced_gmm.reduced_covs)\n",
    "print(\"weight\",reduced_gmm.reduced_weights)\n",
    "print(GMR_greedy(means, covs, weights, 2, \"Salmond\"))\n",
    "print(GMR_greedy(means, covs, weights, 2, \"W\"))\n",
    "print(GMR_greedy(means, covs, weights, 2, \"Runnalls\"))\n",
    "print(GMR_greedy(means, covs, weights, 2, \"Williams\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated dataset 1D\n",
    "\n",
    "np.random.seed(1)\n",
    "def full_mixture(n_comp):\n",
    "    weights, means, sds = np.empty((n_comp,)), np.empty((n_comp,)), np.empty((n_comp,))\n",
    "    for n in range(n_comp):\n",
    "        weights[n] = np.random.uniform(0.05, 0.5)\n",
    "        means[n] = np.random.uniform(0, 3)\n",
    "        sds[n] = np.random.uniform(0.09, 0.5)\n",
    "    weights = weights/weights.sum()\n",
    "    covs = sds**2\n",
    "    return weights, means.reshape((-1,1)), covs.reshape((-1,1,1))\n",
    "\n",
    "\n",
    "# reduce to a 10 component Gaussian mixture\n",
    "n_comps = [40, 120,200, 500, 1000]\n",
    "L = 10\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure case\n",
    "# means = np.array([1,1, 1.00001, 1.0001,1, 1]).reshape((3,2))\n",
    "# covs = np.array([1,0.9,0.9,1]*2+[1,-0.9,-0.9,1]).reshape((3,2,2))\n",
    "# weights = np.array([1/3,1/3,1/3])\n",
    "\n",
    "# print(np.mean(covs,0))\n",
    "\n",
    "# reduced_gmm= GMMReduction(means, covs, weights, 2)\n",
    "# reduced_gmm.lp()\n",
    "# print(\"mean\",reduced_gmm.reduced_means)\n",
    "# print(\"cov\",reduced_gmm.reduced_covs)\n",
    "# print(\"weight\",reduced_gmm.reduced_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array([-20, -0.5]+[0]*10 +[-20,0.5]+[0]*10+[-20,-10]+[0]*10+[-20,10]+[0]*10).reshape((4,12))\n",
    "covs = np.tile(np.eye(12), (4,1,1))\n",
    "weights = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "reduced_gmm= GMMReduction(means, covs, weights, 3)\n",
    "reduced_gmm.lp()\n",
    "print(\"mean\",reduced_gmm.reduced_means)\n",
    "print(\"cov\",reduced_gmm.reduced_covs)\n",
    "print(\"weight\",reduced_gmm.reduced_weights)\n",
    "print(GMR_greedy(means, covs, weights, 3, \"Salmond\"))\n",
    "print(GMR_greedy(means, covs, weights, 3, \"W\"))\n",
    "print(GMR_greedy(means, covs, weights, 3, \"Runnalls\"))\n",
    "print(GMR_greedy(means, covs, weights, 3, \"Williams\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array([0.661,1,1.339,-1,-0.692,1.1,-1.308,-1.1]).reshape((4,2))\n",
    "covs = np.tile(np.eye(2), (4,1,1))\n",
    "weights = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "reduced_gmm= GMMReduction(means, covs, weights, 3)\n",
    "reduced_gmm.lp()\n",
    "print(\"mean\",reduced_gmm.reduced_means)\n",
    "print(\"cov\",reduced_gmm.reduced_covs)\n",
    "print(\"weight\",reduced_gmm.reduced_weights)\n",
    "print(GMR_greedy(means, covs, weights, 3, \"Salmond\"))\n",
    "print(GMR_greedy(means, covs, weights, 3, \"W\"))\n",
    "print(GMR_greedy(means, covs, weights, 3, \"Runnalls\"))\n",
    "print(GMR_greedy(means, covs, weights, 3, \"Williams\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array([0.661,1,1.339,-1,-0.692,1.1,-1.308,-1.1, 0, -10]).reshape((5,2))\n",
    "covs = np.tile(np.eye(2), (5,1,1))\n",
    "weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "reduced_gmm= GMMReduction(means, covs, weights, 3)\n",
    "reduced_gmm.lp()\n",
    "print(\"mean\",reduced_gmm.reduced_means)\n",
    "print(\"cov\",reduced_gmm.reduced_covs)\n",
    "print(\"weight\",reduced_gmm.reduced_weights)\n",
    "\n",
    "#preserve D, E, merge A, B, C\n",
    "\n",
    "\n",
    "means = np.array([0.661,1,1.339,-1,-0.692,1.1,-1.308,-1.1, 0, -10]).reshape((5,2))\n",
    "covs = np.tile(np.eye(2), (5,1,1))\n",
    "weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "reduced_gmm= GMMReduction(means, covs, weights, 3, ground_distance=\"KL\")\n",
    "reduced_gmm.lp()\n",
    "print(\"mean\",reduced_gmm.reduced_means)\n",
    "print(\"cov\",reduced_gmm.reduced_covs)\n",
    "print(\"weight\",reduced_gmm.reduced_weights)\n",
    "print(GMR_greedy(means, covs, weights, 3, \"Salmond\"))\n",
    "print(GMR_greedy(means, covs, weights, 3, \"W\"))\n",
    "print(GMR_greedy(means, covs, weights, 3, \"Runnalls\"))\n",
    "print(GMR_greedy(means, covs, weights, 3, \"Williams\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "means = np.array([1.45, 2.20, 0.67, 0.48, 1.49, 0.91, 1.01, 1.42, 2.77, 0.89]).reshape((-1,1))\n",
    "covs = np.array([0.0487, 0.0305, 0.1171, 0.0174, 0.0295,\n",
    "0.0102, 0.0323, 0.0380, 0.0115, 0.0679]).reshape((-1,1,1))\n",
    "weights = np.array([0.03, 0.18, 0.12, 0.19, 0.02, 0.16, 0.06, 0.1, 0.08, 0.06])\n",
    "reduced_gmm= GMMReduction(means, covs, weights, 5)\n",
    "reduced_gmm.lp()\n",
    "# print(\"mean\",reduced_gmm.reduced_means)\n",
    "# print(\"cov\",reduced_gmm.reduced_covs)\n",
    "# print(\"weight\",reduced_gmm.reduced_weights)\n",
    "\n",
    "mean_S, cov_S, w_S = GMR_greedy(means, covs, weights, 3, \"Salmond\")\n",
    "# print(GMR_greedy(means, covs, weights, 3, \"W\")\n",
    "mean_R, cov_R, w_R =GMR_greedy(means, covs, weights, 3, \"Runnalls\")\n",
    "mean_W, cov_W, w_W = GMR_greedy(means, covs, weights, 3, \"Williams\")\n",
    "# reduced_gmm= GMMReduction(means, covs, weights, 5, ground_distance=\"KL\")\n",
    "# reduced_gmm.lp()\n",
    "# print(\"mean\",reduced_gmm.reduced_means)\n",
    "# print(\"cov\",reduced_gmm.reduced_covs)\n",
    "# print(\"weight\",reduced_gmm.reduced_weights)\n",
    "import scipy.stats as ss\n",
    "\n",
    "xs = np.linspace(-1,4, 200)\n",
    "ys = np.zeros_like(xs)\n",
    "ys_reduced = np.zeros_like(xs)\n",
    "ys_S = np.zeros_like(xs)\n",
    "ys_R = np.zeros_like(xs)\n",
    "ys_W = np.zeros_like(xs)\n",
    "\n",
    "for (l, s), w in zip(zip(means, covs), weights):\n",
    "    ys += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "\n",
    "    \n",
    "for (l, s), w in zip(zip(reduced_gmm.reduced_means, reduced_gmm.reduced_covs), reduced_gmm.reduced_weights):\n",
    "    ys_reduced += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "\n",
    "for (l, s), w in zip(zip(mean_S, cov_S), w_S):\n",
    "    ys_S += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "    \n",
    "for (l, s), w in zip(zip(mean_R, cov_R), w_R):\n",
    "    ys_R += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "\n",
    "for (l, s), w in zip(zip(mean_W, cov_W), w_W):\n",
    "    ys_W += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "    \n",
    "fig, axs = plt.subplots(1, 4, figsize=(16,4))\n",
    "axs[0].plot(xs, ys,label=\"original\")\n",
    "axs[0].plot(xs, ys_reduced,label=\"Our\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(xs, ys,label=\"original\")\n",
    "axs[1].plot(xs, ys_S,label=\"Salmond\")\n",
    "axs[1].legend()\n",
    "axs[2].plot(xs, ys,label=\"original\")\n",
    "axs[2].plot(xs, ys_R,label=\"Runnalls\")\n",
    "axs[2].legend()\n",
    "axs[3].plot(xs, ys,label=\"original\")\n",
    "axs[3].plot(xs, ys_W,label=\"Williams\")\n",
    "axs[3].legend()\n",
    "plt.show()\n",
    "fig.savefig(fname='GMR_reudction_eg1.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "means = np.array([-3.5,-3,-1,0,0.5,2,3,3.5,5,5.5]).reshape((-1,1))\n",
    "covs = np.square(np.array([0.6,0.6,0.6,0.6,0.7,0.7,1,0.5,0.5,0.5]).reshape((-1,1,1)))\n",
    "weights = np.array([0.1]*10)\n",
    "reduced_gmm= GMMReduction(means, covs, weights, 5, ground_distance=\"KL\")\n",
    "reduced_gmm.lp()\n",
    "\n",
    "print(\"mean\",reduced_gmm.reduced_means)\n",
    "print(\"cov\",reduced_gmm.reduced_covs)\n",
    "print(\"weight\",reduced_gmm.reduced_weights)\n",
    "\n",
    "\n",
    "mean_S, cov_S, w_S = GMR_greedy(means, covs, weights, 5, \"Salmond\")\n",
    "mean_R, cov_R, w_R =GMR_greedy(means, covs, weights, 5, \"Runnalls\")\n",
    "mean_W, cov_W, w_W = GMR_greedy(means, covs, weights, 5, \"Williams\")\n",
    "# reduced_gmm= GMMReduction(means, covs, weights, 6, ground_distance=\"KL\")\n",
    "# reduced_gmm.lp()\n",
    "\n",
    "# print(\"mean\",reduced_gmm.reduced_means)\n",
    "# print(\"cov\",reduced_gmm.reduced_covs)\n",
    "# print(\"weight\",reduced_gmm.reduced_weights)\n",
    "\n",
    "xs = np.linspace(-6,8, 200)\n",
    "ys = np.zeros_like(xs)\n",
    "ys_reduced = np.zeros_like(xs)\n",
    "ys_S = np.zeros_like(xs)\n",
    "ys_R = np.zeros_like(xs)\n",
    "ys_W = np.zeros_like(xs)\n",
    "\n",
    "for (l, s), w in zip(zip(means, covs), weights):\n",
    "    ys += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "\n",
    "    \n",
    "for (l, s), w in zip(zip(reduced_gmm.reduced_means, reduced_gmm.reduced_covs), reduced_gmm.reduced_weights):\n",
    "    ys_reduced += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "\n",
    "for (l, s), w in zip(zip(mean_S, cov_S), w_S):\n",
    "    ys_S += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "    \n",
    "for (l, s), w in zip(zip(mean_R, cov_R), w_R):\n",
    "    ys_R += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "\n",
    "for (l, s), w in zip(zip(mean_W, cov_W), w_W):\n",
    "    ys_W += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "    \n",
    "fig, axs = plt.subplots(1, 4, figsize=(16,4))\n",
    "axs[0].plot(xs, ys,label=\"original\")\n",
    "axs[0].plot(xs, ys_reduced,label=\"Our\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(xs, ys,label=\"original\")\n",
    "axs[1].plot(xs, ys_S,label=\"Salmond\")\n",
    "axs[1].legend()\n",
    "axs[2].plot(xs, ys,label=\"original\")\n",
    "axs[2].plot(xs, ys_R,label=\"Runnalls\")\n",
    "axs[2].legend()\n",
    "axs[3].plot(xs, ys,label=\"original\")\n",
    "axs[3].plot(xs, ys_W,label=\"Williams\")\n",
    "axs[3].legend()\n",
    "plt.show()\n",
    "fig.savefig(fname='GMR_reudction_eg2.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "from GaussianMixture_reduction import GMMReduction, GMR_greedy, GMR_Williams\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "means = np.array([1.45, 2.20,0.67,0.48,1.49,0.91,1.01,1.42,2.77,0.89]).reshape((-1,1))\n",
    "covs = np.array([0.048, 0.0305, 0.1171, 0.0174, 0.0295, 0.0102, 0.0323, 0.0380, 0.0115, 0.0679]).reshape((-1,1,1))\n",
    "weights = np.array([0.03, 0.18, 0.12, 0.19, 0.02, 0.16, 0.06, 0.1, 0.08, 0.06])\n",
    "reduced_gmm= GMMReduction(means, covs, weights, 5, ground_distance=\"KL\")\n",
    "reduced_gmm.lp()\n",
    "\n",
    "# print(\"mean\",reduced_gmm.reduced_means)\n",
    "# print(\"cov\",reduced_gmm.reduced_covs)\n",
    "# print(\"weight\",reduced_gmm.reduced_weights)\n",
    "\n",
    "\n",
    "mean_S, cov_S, w_S = GMR_greedy(means, covs, weights, 5, \"Salmond\")\n",
    "mean_R, cov_R, w_R =GMR_greedy(means, covs, weights, 5, \"Runnalls\")\n",
    "mean_W, cov_W, w_W = GMR_greedy(means, covs, weights, 5, \"Williams\")\n",
    "reduced_gmm_williams = GMR_Williams(means, covs, weights, 5, init_method=\"Williams\")\n",
    "reduced_gmm_williams.lp()\n",
    "\n",
    "# print(\"mean\",reduced_gmm.reduced_means)\n",
    "# print(\"cov\",reduced_gmm.reduced_covs)\n",
    "# print(\"weight\",reduced_gmm.reduced_weights)\n",
    "\n",
    "xs = np.linspace(-0.5,3, 200)\n",
    "ys = np.zeros_like(xs)\n",
    "ys_reduced = np.zeros_like(xs)\n",
    "ys_S = np.zeros_like(xs)\n",
    "ys_R = np.zeros_like(xs)\n",
    "ys_W = np.zeros_like(xs)\n",
    "ys_reduced_williams = np.zeros_like(xs)\n",
    "\n",
    "for (l, s), w in zip(zip(means, covs), weights):\n",
    "    ys += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "    \n",
    "for (l, s), w in zip(zip(reduced_gmm.reduced_means, reduced_gmm.reduced_covs), reduced_gmm.reduced_weights):\n",
    "    ys_reduced += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "\n",
    "for (l, s), w in zip(zip(mean_S, cov_S), w_S):\n",
    "    ys_S += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "    \n",
    "for (l, s), w in zip(zip(mean_R, cov_R), w_R):\n",
    "    ys_R += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "\n",
    "for (l, s), w in zip(zip(mean_W, cov_W), w_W):\n",
    "    ys_W += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "    \n",
    "for (l, s), w in zip(zip(reduced_gmm_williams.reduced_means, reduced_gmm_williams.reduced_covs), reduced_gmm_williams.reduced_weights):\n",
    "    ys_reduced_williams += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "\n",
    "    \n",
    "fig, axs = plt.subplots(1, 5, figsize=(20,4))\n",
    "axs[0].plot(xs, ys,label=\"original\")\n",
    "axs[0].plot(xs, ys_reduced,label=\"Our\")\n",
    "axs[0].legend()\n",
    "axs[1].plot(xs, ys,label=\"original\")\n",
    "axs[1].plot(xs, ys_S,label=\"Salmond\")\n",
    "axs[1].legend()\n",
    "axs[2].plot(xs, ys,label=\"original\")\n",
    "axs[2].plot(xs, ys_R,label=\"Runnalls\")\n",
    "axs[2].legend()\n",
    "axs[3].plot(xs, ys,label=\"original\")\n",
    "axs[3].plot(xs, ys_W,label=\"Williams\")\n",
    "axs[3].legend()\n",
    "axs[4].plot(xs, ys,label=\"original\")\n",
    "axs[4].plot(xs, ys_reduced_williams, label = \"ISE\")\n",
    "axs[4].legend()\n",
    "plt.show()\n",
    "fig.savefig(fname='GMR_reudction_eg3.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 1\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "from GaussianMixture_reduction import GMMReduction, GMR_greedy, GMR_Williams\n",
    "\n",
    "means = np.array([1.45, 2.20,0.67,0.48,1.49,0.91,1.01,1.42,2.77,0.89]).reshape((-1,1))\n",
    "covs = np.array([0.048, 0.0305, 0.1171, 0.0174, 0.0295, 0.0102, 0.0323, 0.0380, 0.0115, 0.0679]).reshape((-1,1,1))\n",
    "weights = np.array([0.03, 0.18, 0.12, 0.19, 0.02, 0.16, 0.06, 0.1, 0.08, 0.06])\n",
    "\n",
    "inits = ['Salmond', 'Runnalls', 'Williams', 'W', 'kmeans']\n",
    "n_components = 5\n",
    "\n",
    "reduction_W2 = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"W2\") for init in inits}\n",
    "\n",
    "reduction_KL = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"KL\") for init in inits}\n",
    "\n",
    "reduction_KL_MM = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"KL\", center=\"MM\") for init in inits}\n",
    "\n",
    "reducton_ISE = {init: GMR_Williams(means, covs, weights, n_components, init_method=init) for init in inits}\n",
    "\n",
    "\n",
    "\n",
    "reduction_greedy = {init: GMR_greedy(means, covs, weights, n_components, init) for init in inits}\n",
    "save_file = 'eg1_clustering_greedy.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_greedy, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_W2.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'eg1_clustering_W2.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_W2, f)\n",
    "f.close()\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_KL.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'eg1_clustering_KL.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_KL, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_KL_MM.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'eg1_clustering_KL_MM.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_KL_MM, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reducton_ISE.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'eg1_ISE.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reducton_ISE, f)\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "from GaussianMixture_reduction import GMMReduction, GMR_greedy, GMR_Williams\n",
    "\n",
    "means = np.array([-3.5,-3,-1,0,0.5,2,3,3.5,5,5.5]).reshape((-1,1))\n",
    "covs = np.square(np.array([0.6,0.6,0.6,0.6,0.7,0.7,1,0.5,0.5,0.5]).reshape((-1,1,1)))\n",
    "weights = np.array([0.1]*10)\n",
    "\n",
    "inits = ['Salmond', 'Runnalls', 'Williams', 'W', 'kmeans']\n",
    "n_components = 5\n",
    "\n",
    "reduction_W2 = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"W2\") for init in inits}\n",
    "\n",
    "reduction_KL = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"KL\") for init in inits}\n",
    "\n",
    "reduction_KL_MM = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"KL\", center=\"MM\") for init in inits}\n",
    "\n",
    "reducton_ISE = {init: GMR_Williams(means, covs, weights, n_components, init_method=init) for init in inits}\n",
    "\n",
    "\n",
    "\n",
    "reduction_greedy = {init: GMR_greedy(means, covs, weights, n_components, init) for init in inits}\n",
    "save_file = 'eg2_clustering_greedy.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_greedy, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_W2.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'eg2_clustering_W2.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_W2, f)\n",
    "f.close()\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_KL.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'eg2_clustering_KL.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_KL, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_KL_MM.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'eg2_clustering_KL_MM.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_KL_MM, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reducton_ISE.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'eg2_ISE.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reducton_ISE, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 3\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import  matplotlib.pyplot as plt\n",
    "from GaussianMixture_reduction import GMMReduction, GMR_greedy, GMR_Williams\n",
    "%matplotlib inline\n",
    "\n",
    "means = np.array([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]).reshape((-1,1))\n",
    "covs = np.array([0.01]*10).reshape((-1,1,1))\n",
    "weights = np.array([0.03, 0.18, 0.12, 0.19, 0.02, 0.16, 0.06, 0.1, 0.08, 0.06])\n",
    "\n",
    "\n",
    "xs = np.linspace(-1,2, 200)\n",
    "ys = np.zeros_like(xs)\n",
    "ys_reduced = np.zeros_like(xs)\n",
    "for (l, s), w in zip(zip(means, covs), weights):\n",
    "    ys += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "\n",
    "plt.plot(xs, ys,label=\"original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inits = ['Salmond', 'Runnalls', 'Williams', 'W', 'kmeans']\n",
    "n_components = 3\n",
    "\n",
    "reduction_W2 = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"W2\") for init in inits}\n",
    "\n",
    "reduction_KL = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"KL\") for init in inits}\n",
    "\n",
    "reduction_KL_MM = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"KL\", center=\"MM\") for init in inits}\n",
    "\n",
    "reducton_ISE = {init: GMR_Williams(means, covs, weights, n_components, init_method=init) for init in inits}\n",
    "\n",
    "\n",
    "\n",
    "reduction_greedy = {init: GMR_greedy(means, covs, weights, n_components, init) for init in inits}\n",
    "save_file = 'output/data/eg3_clustering_greedy.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_greedy, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_W2.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg3_clustering_W2.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_W2, f)\n",
    "f.close()\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_KL.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg3_clustering_KL.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_KL, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_KL_MM.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg3_clustering_KL_MM.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_KL_MM, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reducton_ISE.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg3_ISE.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reducton_ISE, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 5\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import  matplotlib.pyplot as plt\n",
    "from GaussianMixture_reduction import GMMReduction, GMR_greedy, GMR_Williams\n",
    "%matplotlib inline\n",
    "\n",
    "# means = np.array([0,0.01,0.02,-0.01,0.012,5,5.01,5.02,-4.98,-4.99]).reshape((-1,1))\n",
    "# covs = np.array([1.1,1,1.2,1,1.1,2.1,2,2.1,2.2,1.9]).reshape((-1,1,1))\n",
    "# weights = np.array([0.25,0.26,0.24,0.2,0.3,0.75,0.74,0.76,0.8,0.7])/5\n",
    "means = np.array([0,0.01,0.02,-0.01,0.012,3,3.01,3.02,3.98,3.99]).reshape((-1,1))\n",
    "covs = np.array([1.1,1,1.2,1,1.1,2.1,2,2.1,2.2,1.9]).reshape((-1,1,1))\n",
    "weights = np.array([0.25,0.26,0.24,0.2,0.3,0.75,0.74,0.76,0.8,0.7])/5\n",
    "\n",
    "\n",
    "xs = np.linspace(-5,10, 200)\n",
    "ys = np.zeros_like(xs)\n",
    "ys_reduced = np.zeros_like(xs)\n",
    "for (l, s), w in zip(zip(means, covs), weights):\n",
    "    ys += ss.multivariate_normal.pdf(xs, mean=l, cov=s) * w\n",
    "\n",
    "plt.plot(xs, ys,label=\"original\")\n",
    "\n",
    "inits = ['Salmond', 'Runnalls', 'Williams', 'W', 'kmeans']\n",
    "n_components = 2\n",
    "\n",
    "reduction_W2 = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"W2\") for init in inits}\n",
    "\n",
    "reduction_KL = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"KL\") for init in inits}\n",
    "\n",
    "reduction_KL_MM = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"KL\", center=\"MM\") for init in inits}\n",
    "\n",
    "reducton_ISE = {init: GMR_Williams(means, covs, weights, n_components, init_method=init) for init in inits}\n",
    "\n",
    "\n",
    "\n",
    "reduction_greedy = {init: GMR_greedy(means, covs, weights, n_components, init) for init in inits}\n",
    "save_file = 'output/data/eg5_clustering_greedy.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_greedy, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_W2.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg5_clustering_W2.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_W2, f)\n",
    "f.close()\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_KL.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg5_clustering_KL.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_KL, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_KL_MM.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg5_clustering_KL_MM.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_KL_MM, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reducton_ISE.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg5_ISE.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reducton_ISE, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 6\n",
    "#2 dimensional case\n",
    "import os\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import  matplotlib.pyplot as plt\n",
    "from GaussianMixture_reduction import GMMReduction, GMR_greedy, GMR_Williams\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "k = np.random.randint(200)\n",
    "nsplit = 2\n",
    "\n",
    "input_folder = '/Users/sarahzhang/ownCloud/Study_Stuff/Research/UBC_PHD/split_conquer_GMM/code/output/save_data/I_2_4'\n",
    "save_file = os.path.join(input_folder, 'case_'+str(k)+'_nsplit_'+str(nsplit)+'_ncomponents_'+str(2)+'_samplesize_'+str(8388608)+'.pickle')\n",
    "infile = open(save_file,'rb')\n",
    "new_dict = pickle.load(infile)\n",
    "infile.close()            \n",
    "means, covs, weights = np.concatenate(new_dict['subset'][0]), np.concatenate(new_dict['subset'][1]), np.concatenate(new_dict['subset'][2])/nsplit\n",
    "\n",
    "\n",
    "inits = ['Salmond', 'Runnalls', 'Williams', 'W', 'kmeans']\n",
    "n_components = 2\n",
    "\n",
    "reduction_W2 = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"W2\") for init in inits}\n",
    "\n",
    "reduction_KL = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"KL\") for init in inits}\n",
    "\n",
    "reduction_KL_MM = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"KL\", center=\"MM\") for init in inits}\n",
    "\n",
    "reducton_ISE = {init: GMR_Williams(means, covs, weights, n_components, init_method=init) for init in inits}\n",
    "\n",
    "\n",
    "\n",
    "# reduction_greedy = {init: GMR_greedy(means, covs, weights, n_components, init) for init in inits}\n",
    "# save_file = 'output/data/eg6_clustering_greedy.pickle'\n",
    "# f = open(save_file, 'wb')\n",
    "# pickle.dump(reduction_greedy, f)\n",
    "# f.close()\n",
    "\n",
    "\n",
    "\n",
    "# for index, (name, estimator) in enumerate(reduction_W2.items()):    \n",
    "#     estimator.lp()\n",
    "# save_file = 'output/data/eg6_clustering_W2.pickle'\n",
    "# f = open(save_file, 'wb')\n",
    "# pickle.dump(reduction_W2, f)\n",
    "# f.close()\n",
    "\n",
    "# for index, (name, estimator) in enumerate(reduction_KL.items()):    \n",
    "#     estimator.lp()\n",
    "# save_file = 'output/data/eg6_clustering_KL.pickle'\n",
    "# f = open(save_file, 'wb')\n",
    "# pickle.dump(reduction_KL, f)\n",
    "# f.close()\n",
    "\n",
    "\n",
    "# for index, (name, estimator) in enumerate(reduction_KL_MM.items()):    \n",
    "#     estimator.lp()\n",
    "# save_file = 'output/data/eg6_clustering_KL_MM.pickle'\n",
    "# f = open(save_file, 'wb')\n",
    "# pickle.dump(reduction_KL_MM, f)\n",
    "# f.close()\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reducton_ISE.items()):   \n",
    "    print(name)\n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg6_ISE.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reducton_ISE, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 6\n",
    "#2 dimensional case\n",
    "import os\n",
    "import _pickle as pickle\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import  matplotlib.pyplot as plt\n",
    "from GaussianMixture_reduction import GMMReduction, GMR_greedy, GMR_Williams\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "k = np.random.randint(200)\n",
    "nsplit = 2\n",
    "\n",
    "input_folder = '/home/qiong/ownCloud/Study_Stuff/Research/UBC_PHD/split_conquer_GMM/code/output/save_data/1d_2c_nonsep'\n",
    "save_file = os.path.join(input_folder, 'case_'+str(k)+'_nsplit_'+str(nsplit)+'_ncomponents_'+str(2)+'_samplesize_'+str(8388608)+'.pickle')\n",
    "infile = open(save_file,'rb')\n",
    "new_dict = pickle.load(infile)\n",
    "infile.close()            \n",
    "means, covs, weights = np.concatenate(new_dict['subset'][0]), np.concatenate(new_dict['subset'][1]), np.concatenate(new_dict['subset'][2])/nsplit\n",
    "\n",
    "\n",
    "inits = ['Salmond', 'Runnalls', 'Williams', 'W', 'kmeans']\n",
    "n_components = 2\n",
    "\n",
    "reduction_W2 = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"W2\") for init in inits}\n",
    "\n",
    "reduction_KL = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"KL\") for init in inits}\n",
    "\n",
    "reduction_KL_MM = {init: GMMReduction(means, covs, weights, n_components, init_method=init, ground_distance=\"KL\", center=\"MM\") for init in inits}\n",
    "\n",
    "reducton_ISE = {init: GMR_Williams(means, covs, weights, n_components, init_method=init) for init in inits}\n",
    "\n",
    "\n",
    "\n",
    "reduction_greedy = {init: GMR_greedy(means, covs, weights, n_components, init) for init in inits}\n",
    "save_file = 'output/data/eg7_clustering_greedy.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_greedy, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_W2.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg7_clustering_W2.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_W2, f)\n",
    "f.close()\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_KL.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg7_clustering_KL.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_KL, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reduction_KL_MM.items()):    \n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg7_clustering_KL_MM.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reduction_KL_MM, f)\n",
    "f.close()\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(reducton_ISE.items()):   \n",
    "    print(name)\n",
    "    estimator.lp()\n",
    "save_file = 'output/data/eg7_ISE.pickle'\n",
    "f = open(save_file, 'wb')\n",
    "pickle.dump(reducton_ISE, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from sklearn import mixture\n",
    "\n",
    "color_iter = itertools.cycle(['navy', 'c', 'cornflowerblue', 'gold',\n",
    "                              'darkorange'])\n",
    "\n",
    "\n",
    "def plot_results(means, covariances, index):\n",
    "    splot = plt.subplot(2, 1, 1 + index)\n",
    "    for i, (mean, covar, color) in enumerate(zip(\n",
    "            means, covariances, color_iter)):\n",
    "        v, w = linalg.eigh(covar)\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        u = w[0] / linalg.norm(w[0])\n",
    "        \n",
    "        # Plot an ellipse to show the Gaussian component\n",
    "        angle = np.arctan(u[1] / u[0])\n",
    "        angle = 180. * angle / np.pi  # convert to degrees\n",
    "        ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "        ell.set_clip_box(splot.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        splot.add_artist(ell)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
